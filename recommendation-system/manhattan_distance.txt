Manhattan distance (MD)
Pros: MD fast and easy to compute.
Cons:    
formula:
    in 2D i.e using x nd y dimensions
    
    | x1 - x2| + | y1 - y2 |

    example:

Given:
customers and book ratings (x,y,z)

customer name, x, y
----------------
Jim,4,1
Bill,5,2
Amy,5,5
Ben,2,4

Calculate Ben's closest neightbor using MD:
Formula:
    
| x1 - x2| + | y1 - y2 |

Step 1:
1. distance between Ben and Jim
|2-4|+|4-1| = |-2|+|3|=5

Step 2: 
2. Distance between Ben and Bill


 |2-5|+|4-2| = |-3|+|2|=5
Step 1:
3. Distance between Ben and Amy

|2-5|+|4-5| = |-3|+|-1|=4

Step 4: Summary
Jim 5
Bill 5
Amy 4

Step 6:Conclusion
Amy is the closest match.

Example application: From machine learning point of view,
we can use Amy's book ratings to recommend books to Ben


Eucledian distance
Based on Pythagorean theorem

formula

sqrt(pow(x1-x2),2)+pow(y1-y2,2))

Data:
name, a,b
-----------
Amy, 5, 5
Ben, 2,4

sqrt((5-2)(5-2)+(5-4)(5-4))

sqrt(10) = 3.16


Summary
Customer, distance from Ben
Amy 3.16
Bill 3.61
Jim 3.61

Manhattan Distance and Euclidean Distance work best when
there are no missing values


Minkowski Distance
A generalization of Manhattan Distance and Euclidean Distance



small example
link
https://www.youtube.com/watch?v=hl3bQySs8sM
p1 = [1,3]
p2=[2,5]
ed=sqrt((p1[0]-[0])**2-(p1[0]-[0])**2)

